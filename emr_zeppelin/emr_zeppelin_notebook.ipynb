{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ced00d",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, regexp_extract\n",
    "from pyspark.sql.types import StringType, FloatType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41113e8",
   "metadata": {},
   "source": [
    "### Get data from S3 and Glue Data Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True)\\\n",
    ".option(\"sep\", \",\")\\\n",
    ".load(\"s3://dirty-transactions-from-csv-to-parquet/dirty_transactions/dirty_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec637653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glue_table = spark.table(\"dirty-transactions-from-csv-to-parquet.dirty_transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0229889",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glue_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda6b4c",
   "metadata": {},
   "source": [
    "### Define UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city_name(string):\n",
    "    cleaned_string = regexp_replace(string, r'[^\\w\\s]', '')\n",
    "    city_name = cleaned_string.strip()\n",
    "    return city_name\n",
    "\n",
    "def extract_only_numbers(string):\n",
    "    numbers = regexp_extract(string, r'\\d+', 0)\n",
    "    return ''.join(numbers)\n",
    "\n",
    "def extract_floats_without_sign(string):\n",
    "    string_without_dollar = regexp_replace(string, r'\\$', '')\n",
    "    return float(string_without_dollar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd9b78",
   "metadata": {},
   "source": [
    "### Register UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"extract_city_name\", extract_city_name, StringType())\n",
    "spark.udf.register(\"extract_only_numbers\", extract_only_numbers, IntegerType())\n",
    "spark.udf.register(\"extract_floats_without_sign\", extract_floats_without_sign, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4c88e3",
   "metadata": {},
   "source": [
    "### Apply functions and create final clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7128eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_glue_table.selectExpr(\n",
    "    \"store_id\",\n",
    "    \"extract_city_name(store_location) as store_location\",\n",
    "    \"product_category\",\n",
    "    \"extract_only_numbers(product_id) as product_id\",\n",
    "    \"extract_floats_without_sign(mrp) as mrp\",\n",
    "    \"extract_floats_without_sign(cp) as cp\",\n",
    "    \"extract_floats_without_sign(discount) as discount\",\n",
    "    \"extract_floats_without_sign(sp) as sp\",\n",
    "    \"date\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0313df",
   "metadata": {},
   "source": [
    "### Write final dataframe to S3 and create a corresponding Data Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f842f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write\\\n",
    ".saveAsTable('dirty-transactions-from-csv-to-parquet.clean_transactions', format='parquet', mode='overwrite',\n",
    "            path='s3://aws-glue-emr-from-csv-to-parquet/clean_transactions_parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
